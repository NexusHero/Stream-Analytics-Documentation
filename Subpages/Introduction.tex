\IEEEraisesectionheading{\section{Einführung}\label{sec:introduction}}

\begin{quote} \textit{\glqq In the Internet of Things […] the real value will remain in the services behind the scenes processing terabytes of data to help people live better lives. \grqq~}\cite{Floarea.2014}\\ \end{quote} 

\IEEEPARstart{D}{er} Hype um den IoT-Bereich geht weiter und die Zahl der Geräte, die Internetzugang haben, wächst kontinuierlich an \cite{peter.2015}. Bereits 2017 sind weltweit 23.35 Milliarden Geräte internetfähig und dies soll - laut Prognosen - bis 2025 auf 74.44 Milliarden ansteigen \cite{Statista.2017}. Das bedeutet, dass die Menge zu verarbeitende Daten ebenfalls ansteigen, sodass eine Datenstromverarbeitung immer wichtiger wird. Datenströme stellen eine nicht-endliche Folge von Datensätzen dar \cite{Mock.2005}. Damit wachsen die Anforderungen für ein Complex Event Processing (CEP) an, sodass die Verarbeitung möglichst in Echtzeit geschieht und beispielsweise kritische Situationen in der Produktion abgefangen werden können \cite{rcrwireless.2016}. Hierfür existieren  bereits einige Datenstromanalyse-Software, welche mit Echtzeitverarbeitung werben: Apache Storm (Opensource-Projekt) \cite{apache.2017} und Microsoft's Stream Analytics(kommerzielles Produkt) \cite{Microsoft.2017}. Laut Microsoft ist ihre Azure Plattform und die darauf angebotenen Dienste eine unglaublich gute Sache. Mit diesem wissenschaftlichen Artikel wird Microsofts Dienst zur Echtzeit-Eventverarbeitung kritisch betrachtet. Er setzt sich mit der Frage auseinander, ab wann es sinnvoll ist auf Microsofts Stream Analytics zurückzugreifen und welche Unterschiede sich hier zur Konkurrenz herausarbeiten lassen. Dabei wird die Leistungsfähigkeit beider Technologien betrachtet … \\ \\
(Text nur rein kopiert) Die zunehmend allgegenwärtigen und leistungsfähigen intelligenten Geräte wie Sensoren und Smartphones fördern die schnelle Entwicklung von Datenstreaming-Anwendungen wie Ereignusüberwachung. Die massiven Datenströme, die durch diese Anwendungen erzeugt werden, haben das Internet der Dinge (loT) zu einer wichtigen Quelle für Big Data gemacht. 
Ereignisgesteuerte Informationssysteme, die einen Strom von Sensordaten auswerten müssen, brauchen eine automatische Verarbeitung dieser Daten. Genau hier setzt Real-Time Stream Analytics an. Der Ansatz leitet aus einem nicht endenden Strom aus Echtzeit Sensordaten Muster und komplexe Ereignisse ab, wodurch es einem System ermöglicht wird, auf Situationen entsprechende Maßnahmen anzuwenden und so geeignet darauf zu reagieren. Nicht nur Microsoft bietet in diesem Bereich eine Lösung. Kosten- und lizenzfreie Produkte wie Apache Storm bieten hier eine echte Alternative. 
Da Unternehmen meist Daten analysieren die offline vorliegen, also quasi das „Kind schon in den Brunnen gefallen ist“, ist die Auswertung von Datenströmen zur Laufzeit enorm wichtig. Mögliche Einsatzgebiete sind dabei beispielsweise elektronischer Handel, Logistik, Netzwerküberwachung usw. 
Mit dieser Technologie ist es also möglich auf eine Reihe von Ergebnissen zu reagieren, die soeben stattgefunden haben. Dabei hat man eine Verbindung zu externen Datenquellen, sodass die Anwendungen eingehende Daten verwenden können. Des geschieht dann über gewöhnliche Datenbankabfragen, nur dass hier der FROM-Teil keiner Relation entspricht, sondern einem Datenstrom. [1]
Dieser Artikel präsentiert eine systemische Studie zur Verarbeitung und Analyse von Datenströmen im Kontext

