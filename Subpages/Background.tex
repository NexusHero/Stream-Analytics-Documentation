\section{Stream Analytics und Complex Event Processing}
Stream Analytics beschreibt das analysieren und verarbeiten von Datenströmen. Das allerdings zur Laufzeit nahezu in Echtzeit. Echtzeitverarbeitung bezeichnet die Verarbeitung eines unendlichen Stroms von Eingabedaten, wobei die Zeit zwischen Datenaufnahme und den fertigen Ergebnissen sehr gering ist. Typischerweise wird dies in Sekunden gemessen. Grundsätzlich geht es bei der Verarbeitung um filtern, aggregieren und das Suchen von Msutern. Die Resultate lassen sich anschließend an entsprechende Anwendungen weiterleiten.  Stream Analytics gehört somit zum Themenbereich von Complex Event Processing. 
Complex Event Processing (kurz CEP) ist ein Sammelbegriff, welcher Methoden, Techniken und verschiedene Werkzeuge umfasst, mit denen Ereignisse verarbeitet werden können, während sie passieren. Dieser Bereich befasst sich somit kontinuierlich und zeitnah mit der Erkennung, Analyse und Verarbeitung voneinander unabhängiger Ereignisse. Hierbei wird aus dem eingehenden Datenstrom in Echtzeit Wissen abgeleitet, um auf Situationen entsprechend zu reagieren. Typische Reaktionen wären beispielsweise Benachrichtigungen, einfache Aktionen oder Interaktionen mit Geschäftsprozessen. 
Dabei werden kontinuierliche Ereignisabfragen ausgewertet. Die Ergebnisse enthalten Daten, die für die Reaktion relevant sind. Es können mehrere einzelne Ereignisse verbunden werden, sodass sie ein komplexes Ereignis ergeben. Für verschiedene Situationen ist es zudem wichtig, dass Ereignisse in einem gewissen Zeitraum bzw. einer bestimmten Reihenfolge auftreten. Somit sind zeitliche Zusammenhäng ebenfalls von Bedeutung. Da der Ereignisstrom quasi unendlich ist, können nur auf bestimmte Ausschnitte (Fenster) gewisse Anfragen gestellt werden. [2]
Da Datenströme häufig aus einem großen teil irrelevanter Daten („noise“) besteht und nur zu einem Bruchteil aus Nutzdaten („signal“), müssen diese Wichtigen Daten über geeignete Filtermechanismen herausgefiltert werden. Dies ermöglicht eine Konzentration der relevanten Anteile, während beispielsweise Messfehler direkt aussortiert werden können. Nachfolgend werden die relevanten Teile mit anderen Datenströmen in Bezug gebracht. Durch diese Korrelation der einzelnen Sensordaten können Situationen erstellt werden. Zusätzlich zu dieser Korrelation können die Daten mit Informationen aus unterschiedlichen Datenbanken angereichert werden. Dies kann nützlich sein, um noch aussagekräftigere Ergebnisse zu erzielen. Dabei ist vor allem die zeitliche Komponente der Ereignisse ein wichtiger Faktor. Da Ereignisse, die in einem bestimmten Zeitraum passieren von großer Bedeutung sein können, gibt es die Funktion der Zeitfenster, wodurch auf einen gewissen Abschnitt Funktionen angewandt werden können. Nach dieser Vorverarbeitung des Ereignisstroms lassen sich die Daten gezielt nach definierten Mustern absuchen. Werden entsprechende Muster gefunden, kann das System darauf reagieren. Hier liegt der eigentliche Wert der Technologie [5].



% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi